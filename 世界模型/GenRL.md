- GenRL 通过学习一个**任务无关的MFWM**（Multimodal-foundation world model）来连接和对齐**基础VLM的嵌入空间**与**生成式世界模型**的**潜在空间**
-  一旦MFWM训练完成，GenRL 就可以将**语言或视觉提示**（prompts）转换为**潜在目标**。然后，RL 代理可以在世界模型的“想象”中学习实现这些目标，通过**轨迹匹配**（trajectory matching）来最小化策略（policy）与目标轨迹之间的差异

**MFWM 的整体工作流程：**
- **预训练阶段：**
    1. 首先，独立地预训练一个**生成式世界模型**，使其能够学习环境的动态，并将其编码到潜在空间 `S`。
    2. 然后，利用预训练的 VLM，获取大量的视觉数据 `x_{t:t+k}` 的视觉嵌入 `e(v)`。
    3. **连接器**被训练来将 VLM 的视觉嵌入 `e(v)` 映射到世界模型的潜在状态 `s_t`。
    4. **对齐器**被训练来将 VLM 视觉嵌入 `e(v)` 周围的采样点拉近 `e(v)`，从而间接解决多模态对齐问题。（对齐器网络接收 VLM 的语言嵌入 `e(l)` 作为输入，并输出一个经过对齐的语言嵌入 `e(l)'`，最小化对齐后的语言嵌入 `e(l)'` 与原始视觉嵌入 `e(v)` 之间的 L2 距离来训练）
- **任务指定和行为学习阶段：**
    1. 当接收到一个**语言或视觉提示**（task prompt）时，VLM 的语言/视觉嵌入器将其转换为嵌入 `e_{task}`。
    2. **连接器**将 `e_{task}`（如果是视觉提示，则是其视觉嵌入；如果是语言提示，也通过视觉嵌入器处理，或者通过一个单独的对齐器处理后输入连接器）转换为一系列**潜在目标状态** `c_{task}`。
    3. **生成式世界模型**利用这些潜在目标状态 `c_{task}`，通过其动态模型**想象**（imagine）出轨迹。
    4. 一个**策略模型**（policy model）被训练来最大化一个**奖励函数**（如论文中的 Eq. 3），该奖励函数衡量了策略产生的轨迹（智能体在想象空间实际执行的轨迹）与由提示生成的潜在目标轨迹（根据提示期望达成的轨迹）之间的**相似性**。

