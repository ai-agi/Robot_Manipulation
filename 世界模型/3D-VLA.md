#### **1. 3D 感知与特征提取 (Perception & Feature Extraction)**

- **输入：**
    - 多视图 RGB-D 图像（或直接 3D 点云数据）。
    - 用户输入的文本指令（如“Find some snacks for me.”）。
- **处理模型：**
    - **Q-Former (或类似视觉编码器)：** 将多视图图像等 2D 视觉特征转化为统一的 3D 场景特征。
    - **3D-LLM Backbone (BLIP2-FlanT5XL 基础上构建)：** 接收文本指令，并结合 Q-Former 提取的 3D 视觉特征，将其融入到 LLM 的嵌入空间中。
- **输出：**
    - 编码后的 **3D 场景特征嵌入**（被 `<scene> </scene>` 令牌封装）。
    - 处理后的**文本指令嵌入**。

#### **2. 核心语言理解与推理 (Language Understanding & Reasoning)**

- **输入：**
    - **3D 场景特征嵌入**（来自感知阶段）。
    - **用户文本指令嵌入**（来自感知阶段）。
    - **交互令牌：** 如 `<obj> </obj>` (物体)、`<loc0-255>` (位置) 等，这些在训练时被 LLM 学习理解其语义。
- **处理模型：**
    - **3D-LLM Backbone (基于 BLIP2-FlanT5XL 的主要 LLM 部分)：** 这是一个强大的生成式语言模型，具备 3D 理解能力，能够处理交织的 3D 场景特征和文本。
- **输出 (取决于任务)：**
    - **推理结果：** 对 3D 场景的问答（Embodied QA, What-if QA）、任务描述（Task Caption）、物体定位（Localization）、稠密描述（Dense Caption）等。这些输出通常是**文本形式**，包含或生成相应的交互令牌来指代 3D 实体。
    - **目标状态的文本描述：** 如果是目标生成任务，LLM 会生成一个包含指令和**模态指示令牌**（如 `<image> </image>` 或 `<pcd> </pcd>`）的文本，描述期望的最终状态。

#### **3. 多模态目标生成 (Multimodal Goal Generation)**

- **输入：**
    - LLM 生成的包含**模态指示令牌**（`<image>` 或 `<pcd>`) 和文本指令的**目标文本描述**。
    - LLM 的**输出特征**（代表目标指令和场景理解）。
- **处理模型：**
    - **Projector (投影器)：** 将 LLM 的输出特征和嵌入空间对齐到扩散模型的输入空间。
    - **预训练的具身扩散模型 (Embodied Diffusion Models)：**
        - **RGBD-to-RGBD 扩散模型：** 用于生成目标图像（RGB）和深度图（D）。
        - **Point-to-Point 扩散模型：** 用于生成目标点云。
        - 这些扩散模型是条件性的，以初始状态模态和 LLM 给出的指令为条件进行生成。
- **输出：**
    - **目标图像 (RGB-D)** 或 **目标点云 (Point Cloud)**。这些是模型“想象”出的未来状态的视觉表示。

#### **4. 机器人动作规划 (Robot Action Planning)**

- **输入：**
    - **当前 3D 场景特征嵌入**（来自感知阶段）。
    - **用户指令**（如“Execute now.”）。
    - **目标状态的文本描述**（如果之前进行了目标生成）。
    - **想象出的目标图像/点云**（通过 Projector 转换回 LLM 可理解的特征，或作为引导信号）。
- **处理模型：**
    - **3D-LLM Backbone (主要 LLM 部分)：** 结合当前场景、用户指令和想象的目标状态，进行高级别的规划和决策。
- **输出：**
    - **机器人动作令牌序列**（如 `<aloc>`, `<arot>`, `<gripper>` 等），这些是机器人执行任务所需的具体操作指令。